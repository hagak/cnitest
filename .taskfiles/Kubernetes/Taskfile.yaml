---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: "3"

tasks:

  browse-pvc:
    desc: Browse PersistentVolumeClaims
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        ns: Namespace to browse PersistentVolumeClaims in (default: default)
        claim: PersistentVolumeClaim to browse (required)
    interactive: true
    cmd: kubectl browse-pvc --context {{.cluster}} --namespace {{.ns}} {{.claim}}
    vars:
      ns: '{{.ns | default "default"}}'
    requires:
      vars: ["cluster", "claim"]

  drain:
    desc: Drain a node
    summary: |
      Args:
        cluster: Cluster to run command against (required)
        node: Node to drain (required)
    cmd: kubectl --context {{.cluster}} drain {{.node}} --ignore-daemonsets --delete-local-data --force
    requires:
      vars: ["cluster", "node"]

  delete-failed-pods:
    desc: Deletes pods with a fucked status
    summary: |
      Args:
        cluster: Cluster to run command against (required)
    cmds:
      - for: ["Evicted", "Failed", "Succeeded"]
        cmd: kubectl --context {{.cluster}} delete pods --field-selector status.phase={{.ITEM}} -A --ignore-not-found=true
    requires:
      vars: ["cluster"]

  clear-multus:
    desc: Delete multus host files
    cmds:
      - kubectl --context {{.cluster}} debug -n kube-system k3snode01

  # Below is needed due to coredns restarts and these deployments having issues afterwards
  restart-key-deployments:
    desc: Restart deployments that go bad after coredns restart
    cmds:
      - kubectl --context {{.cluster}} rollout restart deployment/loki-gateway -n monitoring
      - kubectl --context {{.cluster}} rollout restart deployment/mailu-rspamd -n mailu
      - kubectl --context {{.cluster}} rollout restart deployment/mailu-front -n mailu
      - kubectl --context {{.cluster}} rollout restart deployment/nginx-internal-controller -n network
      - kubectl --context {{.cluster}} rollout restart deployment/nginx-external-controller -n network
